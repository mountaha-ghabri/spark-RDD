{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ldnZ2q1LurTl"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "name": "BE4-Spark.ipynb"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mountaha-ghabri/spark-RDD/blob/main/spark_core_api_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXigF59Q-8aH"
      },
      "source": [
        "<center><img src='https://netacad.centralesupelec.fr/img/cs.jpg' width=200></center>\n",
        "\n",
        "\n",
        "<hr color=\"black\">\n",
        "\n",
        "<center><h1>1. Introduction to the Spark Core API</h1></center>\n",
        "\n",
        "<hr color=\"black\">\n",
        "\n",
        "\n",
        "Spark is a distributed computing framework designed to be fast and general-purpose.\n",
        "It is used to develop applications that can execute **parallel computations** on data distributed across servers in a **cluster**.\n",
        "\n",
        "Spark provides several APIs (Application Programming Interfaces) for the implementation of parallel computations.\n",
        "\n",
        "This tutorial introduces the low-level **Spark Core API**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "__Documentation.__ For further information, please refer to the [official documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execute the following cell to install the necessary tools to run Spark code.**\n",
        "\n",
        "â° The cell will run for about 1 minute and 30 seconds."
      ],
      "metadata": {
        "id": "o3ndrzJnhtE2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giAUTZyo-8aJ"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "!java -version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execute the following cell to initialize the SparkContext and download some datasets.**"
      ],
      "metadata": {
        "id": "QzxToTAniATJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVfhMGXz5Qel"
      },
      "source": [
        "import pyspark\n",
        "import random\n",
        "sc = pyspark.SparkContext(appName=\"code-tutorial-1\")\n",
        "print(\"Initialization successful\")\n",
        "\n",
        "!wget -q https://gquercini.github.io/courses/plp/tutorials/data.tgz\n",
        "!tar xf data.tgz\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resilient Distributed Datasets (RDDs)\n",
        "\n",
        "Data distributed across servers in a cluster is represented by the Spark Core API through a data structure called Resilient Distributed Dataset (RDD).\n",
        "\n",
        "A **Resilient Distributed Dataset (RDD)** is an immutable, distributed collection of items (e.g., integer, strings, objects).\n",
        "\n",
        "The items of an RDD are organized into distinct **partitions**.\n",
        "Each partition is stored independently on a server of the cluster;\n",
        "two distinct partitions may reside on the same server.\n",
        "\n"
      ],
      "metadata": {
        "id": "G9S-dlTiitz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two ways exist to create a RDD:\n",
        "\n",
        "* from a collection of elements in **main memory**.\n",
        "\n",
        "* from an **external data source**, such as a file, a collection of files or a database."
      ],
      "metadata": {
        "id": "bdFNzgsLgSsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RDDs from in-memory collections\n",
        "\n",
        "Lists, sets, dictionaries are example of collections whose items reside in main memory.\n",
        "\n",
        "We use the function ``parallelize()`` to create an RDD from an in-memory collection."
      ],
      "metadata": {
        "id": "Q9IbQx-_i3OH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "autoscroll": "json-false",
        "ein.tags": [
          "worksheet-0"
        ],
        "scrolled": true,
        "id": "NZpudf5li3OP"
      },
      "source": [
        "# A Python list.\n",
        "men_list = ['mark', 'anthony', 'nick', 'charles', 'seth', 'joe', 'matthew', 'michael', 'andrea']\n",
        "\n",
        "# Create a RDD from the list\n",
        "men_rdd = sc.parallelize(men_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get the number of partitions of the new RDD with the function ``getNumPartitions()`` and the content of each partition with the function ``glom()``.\n",
        "\n",
        "ðŸ‘‰ Knowing the content of the partitions is not necessary to implement Spark computations. This is only presented here to show what is under the hood."
      ],
      "metadata": {
        "id": "DMWUBX_H0emO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of partitions of men_rdd: {men_rdd.getNumPartitions()}\")\n",
        "\n",
        "# We'll explain the function collect later.\n",
        "for i, partition in enumerate(men_rdd.glom().collect()):\n",
        "  print(f\"Content of partition {i}: {partition}\")"
      ],
      "metadata": {
        "id": "L-ggeGg60zB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "ðŸ¤” **How is the number of partitions determined?**\n",
        "\n",
        "* When we run Spark on a single machine (as is our case), the number of partitions equals the **number of CPU cores** of the local machine. If the local machine has only one core, Spark creates 2 partitions.\n",
        "\n",
        "* When we run Spark on a cluster, and we call ``parallelize()`` on a collection, the contents of the collection reside in the main memory of the server where ``parallelize()`` is called. If the collection is large, Spark splits the collection up into as many partitions as the **number of CPU cores across all servers** in the cluster. Then it distributes the partitions across these servers.\n",
        "For small collections, Spark only creates two partitions and keeps them on a single server.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "26DbzU6d0yOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‰ Here our code is **run in a virtual machine** on Google Colab.\n",
        "\n",
        "**The code is not run on your local machine.**\n",
        "\n",
        "ðŸ’¡ Do you want to know **how many cores** the virtual machine has? **Execute the following command.**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y1hGGq8sUswp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nproc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUV1Kn_VVK_V",
        "outputId": "e7046226-e005-413d-b830-c50764b57185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RDDs from external data sources\n",
        "\n",
        "Spark can create RDDs from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra (a NoSQL database), HBase (the Hadoop database), Amazon S3 (a Cloud storage service). Spark supports text files, sequence files (essentially, binary files containing key/value pairs) and any Hadoop input format.\n",
        "\n",
        "\n",
        "Different functions exist to create an RDD from an external source (``textFile()``, ``sequenceFile()``, ``pickleFile()``...).\n",
        "\n",
        "The function ``textFile()`` is used to create a RDD from the content of a text file.\n",
        "\n",
        "ðŸ‘‰ Each item of the new RDD is a line of the file.\n",
        "\n"
      ],
      "metadata": {
        "id": "gelfqLS5i3OP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = \"./data/moby-dick.txt\"\n",
        "lines_rdd = sc.textFile(data_file)\n",
        "\n",
        "# The function take() is an action that shows the first 20 items of the RDD.\n",
        "# We'll study actions in the next sections.\n",
        "lines_rdd.take(20)"
      ],
      "metadata": {
        "id": "GXYyir6ri3OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "ðŸ¤” **How is the number of partitions determined?**\n",
        "\n",
        "* When we run Spark on a single machine (as is our case), the number of partitions equals the **number of CPU cores** of the local machine. If the local machine has only one core, Spark creates 2 partitions.\n",
        "\n",
        "* When we run Spark on a cluster, and we open a file stored in a distributed file system, such as HDFS, Spark creates an RDD with as many partitions as the **number of blocks** of the file. Note that these blocks are already distributed across the servers of the cluster.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "EsOOhOgqc8g9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‰ Two types of functions can be called on a RDD: **transformations** and **actions**."
      ],
      "metadata": {
        "id": "eE6yiqqeje7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformations\n",
        "\n",
        "A **transformation** is a function that takes in one or more RDDs and returns a new RDD.\n",
        "\n",
        "A transformation applies a computation on the input RDD. The execution of a transformation triggers the creation of as many computational tasks as the number of partitions in the RDD. Each task applies the computation on a specific partition. Since partitions are disjoint, these tasks can run in parallel.\n"
      ],
      "metadata": {
        "id": "1fSjpAAtZ3JL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create an RDD and study common transformations."
      ],
      "metadata": {
        "id": "np652x3QePiY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "autoscroll": "json-false",
        "ein.tags": [
          "worksheet-0"
        ],
        "id": "EZ7tqcs7-8aN",
        "scrolled": true
      },
      "source": [
        "# A Python list.\n",
        "men_list = ['mark', 'anthony', 'nick', 'charles', 'seth', 'joe', 'matthew', 'michael', 'andrea']\n",
        "\n",
        "# Create a RDD from the list\n",
        "men_rdd = sc.parallelize(men_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformation ``map()``\n",
        "\n",
        "The transformation ``map()`` takes in an RDD $R$ and a function $f$ and returns a new RDD by applying $f$ to each element of $R$."
      ],
      "metadata": {
        "id": "jnkFLIUlbKJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Input.__  \n",
        "\n",
        "* A RDD $R = [r_i | 0\\leq i \\leq k-1]$.\n",
        "* A function $f$.\n",
        "\n",
        "__Output.__\n",
        "\n",
        "* A __new RDD__ $R^\\prime = [f(r_i) | r_i \\in R]$.\n"
      ],
      "metadata": {
        "id": "qIM9FUIIml-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following example, the function $f$ is specified as an anonymous function (a *lambda function*): the function takes in a single item of the input RDD and returns a value."
      ],
      "metadata": {
        "id": "mohuzNginzMM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbGpHrXJQdZe"
      },
      "source": [
        "# collect() is an action that returns a list from the given RDD.\n",
        "print(f\"Content of the input RDD: {men_rdd.collect()}\")\n",
        "\n",
        "men_rdd_map = men_rdd.map(lambda name: name.capitalize())\n",
        "\n",
        "print(f\"Content of the output RDD: {men_rdd_map.collect()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also specify $f$ as a regular Python function and pass it to ``map()`` as an argument."
      ],
      "metadata": {
        "id": "-y-Cswz41a8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def capitalize_name(name):\n",
        "  return name.capitalize()\n",
        "\n",
        "# collect() is an action that returns a list from the given RDD.\n",
        "print(f\"Content of the input RDD: {men_rdd.collect()}\")\n",
        "\n",
        "men_rdd_map = men_rdd.map(capitalize_name)\n",
        "\n",
        "print(f\"Content of the output RDD: {men_rdd_map.collect()}\")"
      ],
      "metadata": {
        "id": "MXBVTY3o1hUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following example uses the RDD ``lines_rdd`` that we created above.\n",
        "\n",
        "ðŸ‘‰ Each item of ``lines_rdd`` is a line from a file.\n",
        "\n",
        "ðŸ‘‰ The transformation ``map()`` applies the function ``split()`` to each item of ``lines_rdd``.\n",
        "\n",
        "ðŸ‘‰ An item of the RDD returned by the transformation ``map()`` is the list of words occurring in a line.\n"
      ],
      "metadata": {
        "id": "KJLubzuboq_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Content of the input RDD: {lines_rdd.take(8)}\")\n",
        "\n",
        "lines_rdd_map = lines_rdd.map(lambda line: line.split())\n",
        "\n",
        "print(f\"Content of the output RDD: {lines_rdd_map.take(8)}\")"
      ],
      "metadata": {
        "id": "wzsBzQf3on7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformation ``flatMap()``\n",
        "\n",
        "The transformation ``flatMap()`` does the same thing as the transformation ``map()``, with the following difference. If each element of the RDD obtained from applying ``map()`` is a collection (a list, a set, or a tuple), the application of ``flatMap()`` \"flattens\" the collection.\n",
        "\n"
      ],
      "metadata": {
        "id": "T7OkEB07pEYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the last example again.\n",
        "The Python function ``split()`` splits a given string into its constituent words; it returns a list."
      ],
      "metadata": {
        "id": "6JawkXPkpIV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Content of the input RDD: {lines_rdd.take(8)}\")\n",
        "\n",
        "lines_rdd_map = lines_rdd.map(lambda line: line.split())\n",
        "\n",
        "print(f\"Content of the output RDD (after map): {lines_rdd_map.take(8)}\")"
      ],
      "metadata": {
        "id": "_X2su0f-pU58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, if we want each item of  the output RDD to be a single word (in other words, we want to flatten each list), we can use ``flatMap()``."
      ],
      "metadata": {
        "id": "Jq6lJXnTp13z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines_rdd_flatmap = lines_rdd.flatMap(lambda line: line.split())\n",
        "print(f\"Content of the output RDD (after flatMap): {lines_rdd_flatmap.take(10)}\")"
      ],
      "metadata": {
        "id": "EFj4sWJ-nzIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pivVPInlRJTw"
      },
      "source": [
        "## Transformation ``filter()``\n",
        "\n",
        "The transformation ``filter()`` takes in an RDD $R$ and a predicate $p$ and returns a new RDD containing only the elements of $R$ that satisfy $p$.\n",
        "\n",
        "ðŸ‘‰ A **predicate** is a function that returns a Boolean value."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Input.__  \n",
        "\n",
        "* A RDD $r = [r_i | 0\\leq i \\leq k-1]$.\n",
        "* A predicate $p$.\n",
        "\n",
        "__Output.__\n",
        "\n",
        "* A __new RDD__ $R^\\prime = [r_i | r_i \\in R \\land p(r_i)]$."
      ],
      "metadata": {
        "id": "9ZHdXf-xqGeS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blhMBJqZ-8aQ"
      },
      "source": [
        "print(f\"Content of the input RDD: {men_rdd_map.collect()}\")\n",
        "\n",
        "men_rdd_filter = men_rdd_map.filter(lambda name: name[0] ==\"M\")\n",
        "\n",
        "print(f\"Content of the output RDD (after filter): {men_rdd_filter.collect()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrQmBzVpT_Le"
      },
      "source": [
        "## Transformation ``union()``\n",
        "\n",
        "The transformation ``union()`` takes in two RDDs and returns a new RDD representing the union of the two input RDDs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "__Input.__  \n",
        "\n",
        "* A RDD $R = [r_i | 0\\leq i \\leq k-1]$.\n",
        "* A RDD $S = [s_i | 0\\leq i \\leq m-1]$.\n",
        "\n",
        "__Output.__\n",
        "\n",
        "* A __new RDD__ $U = [u_i | u_i \\in R \\lor u_i \\in S]$.\n",
        "\n",
        "ðŸ‘‰ Unlike for the corresponding mathematical operation, the result of the transformation ``union()`` may contain duplicate values (e.g., Andrea in our example)."
      ],
      "metadata": {
        "id": "zapuF2EPrftq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLmyRepNUJsn"
      },
      "source": [
        "women_rdd = sc.parallelize(['Anna', 'Laura', 'Emma', 'Charlotte', 'Sophia', 'Isabella', 'Mia', 'Andrea'])\n",
        "\n",
        "print(f\"Content of the first input RDD: {men_rdd_map.collect()}\")\n",
        "\n",
        "print(f\"Content of the second input RDD: {women_rdd.collect()}\")\n",
        "\n",
        "people_rdd = women_rdd.union(men_rdd_map)\n",
        "\n",
        "print(f\"Content of the output RDD (after union): {people_rdd.collect()}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmwDxlqMTgQr"
      },
      "source": [
        "## Transformation ``distinct()``\n",
        "\n",
        "The transformation ``distinct()`` takes in an RDD and returns a new RDD containing the distinct elements in the input RDD."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Input.__  \n",
        "\n",
        "* A RDD $R = [r_i | 0\\leq i \\leq k-1]$.\n",
        "\n",
        "__Output.__\n",
        "\n",
        "* A __new RDD__ $R^\\prime = [r_i | r_i \\in R \\land (r_i = r_j \\implies i = j)]$.\n",
        "\n",
        "ðŸ‘‰ Note that the items in the output RDD are not in the same order as in the input RDD.\n",
        "This is due to the fact that the transformation ``distinct()`` shuffles the elements so that duplicates will be in the same partition."
      ],
      "metadata": {
        "id": "pQquxeNhstHu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWlz9DX3Tpo7"
      },
      "source": [
        "print(f\"Content of the input RDD: {people_rdd.collect()}\")\n",
        "\n",
        "people_rdd_distinct = people_rdd.distinct()\n",
        "\n",
        "print(f\"Content of the output RDD (after distinct): {people_rdd_distinct.collect()}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FliToid4Ua-e"
      },
      "source": [
        "## Transformation ``intersection()``\n",
        "\n",
        "The transformation ``intersection()`` takes in two RDDs and returns a new RDD that contains the items that are common to both RDDs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Input.__  \n",
        "\n",
        "* A RDD $R = [r_i | 0\\leq i \\leq k-1]$.\n",
        "* A RDD $S = [s_i | 0\\leq i \\leq m-1]$.\n",
        "\n",
        "__Output.__\n",
        "\n",
        "* A __new RDD__ $T = [t_i | t_i \\in R \\land t_i \\in S \\land (t_i = t_j \\implies i = j)]$.\n",
        "\n",
        "\n",
        "ðŸ‘‰ The output does not contain any duplicate elements, even if the input RDDs did.\n"
      ],
      "metadata": {
        "id": "RzEIfWtBtdab"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tod9wljeUfPO"
      },
      "source": [
        "print(f\"Content of the first input RDD: {people_rdd.collect()}\")\n",
        "\n",
        "print(f\"Content of the second input RDD: {women_rdd.collect()}\")\n",
        "\n",
        "intersect_rdd = people_rdd.intersection(women_rdd)\n",
        "\n",
        "print(f\"Content of the output RDD (after intersect): {intersect_rdd.collect()}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvYw1te3Uppi"
      },
      "source": [
        "## Transformation ``subtract()``\n",
        "\n",
        "The transformation ``subtract()`` takes in two RDDs and returns a new RDD containing the items of the first RDD that are not in the second RDD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Input.__  \n",
        "\n",
        "* A RDD $R = [r_i | 0\\leq i \\leq k-1]$.\n",
        "* A RDD $S = [s_i | 0\\leq i \\leq m-1]$.\n",
        "\n",
        "__Output.__\n",
        "\n",
        "* A __new RDD__ $T = [t_i | t_i \\in R \\land t_i \\notin S]$.\n"
      ],
      "metadata": {
        "id": "YqNN8lHzuRRu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qRn-_atUuT9"
      },
      "source": [
        "print(f\"Content of the first input RDD: {people_rdd.collect()}\")\n",
        "\n",
        "print(f\"Content of the second input RDD: {men_rdd_map.collect()}\")\n",
        "\n",
        "subtract_rdd = people_rdd.subtract(men_rdd_map)\n",
        "\n",
        "print(f\"Content of the output RDD: {subtract_rdd.collect()}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OrvvBAmU7WT"
      },
      "source": [
        "## Transformation ``cartesian()``\n",
        "\n",
        "The transformation ``cartesian()`` takes in two RDDs and returns a new RDD that is the Cartesian product of the two input RDDs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Input.__  \n",
        "\n",
        "* A RDD $R = [r_i | 0\\leq i \\leq k-1]$.\n",
        "* A RDD $S = [s_i | 0\\leq i \\leq m-1]$.\n",
        "\n",
        "__Output.__\n",
        "\n",
        "* A __new RDD__ $T = [(r_i, s_i) | \\forall r_i \\in R\\ \\forall s_i \\in S]$."
      ],
      "metadata": {
        "id": "0kqILAFru_v-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7vD61cFVBLN"
      },
      "source": [
        "print(f\"Content of the first input RDD: {men_rdd_map.collect()}\")\n",
        "\n",
        "print(f\"Content of the second input RDD: {women_rdd.collect()}\")\n",
        "\n",
        "cartesian_rdd = men_rdd_map.cartesian(women_rdd)\n",
        "\n",
        "print(f\"Content of the output RDD: {cartesian_rdd.collect()}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actions\n",
        "\n",
        "An **action** is a function that takes in a RDD and returns a value, other than a RDD.\n",
        "\n",
        "An action is applied to each partition of the input RDD. The value obtained from each partition is sent to the driver of the Spark application.\n",
        "\n",
        "ðŸ‘‰ As a result, an action always generates some traffic on the network, as the result obtained from each partition is sent back to the server where the driver is running."
      ],
      "metadata": {
        "id": "rWLy0NiqlTqg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfdRwn1FWm2E"
      },
      "source": [
        "## Action ``count()``\n",
        "\n",
        "The action ``count()`` takes in a RDD and returns the number of elements in the RDD.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Input.__  \n",
        "\n",
        "* A RDD $R = [r_i | 0\\leq i \\leq k-1]$.\n",
        "\n",
        "__Output.__\n",
        "\n",
        "* The number $k$ of items in $R$.\n"
      ],
      "metadata": {
        "id": "Nmh85Fo_5Ym_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ubgh3QrHWuUE"
      },
      "source": [
        "people_rdd.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym189mtHWxto"
      },
      "source": [
        "## Action ``first()``\n",
        "\n",
        "The action ``first()`` takes in an RDD and returns the first element of the RDD.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "__Input.__  \n",
        "\n",
        "* A RDD $R = [r_i | 0\\leq i \\leq k-1]$.\n",
        "\n",
        "__Output.__\n",
        "\n",
        "* $r[0]$, the first item in $R$.\n",
        "\n"
      ],
      "metadata": {
        "id": "PdoQmd485onc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X25dUiofW-2N"
      },
      "source": [
        "people_rdd.first()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoUmriPYXCu-"
      },
      "source": [
        "## Action ``take()``\n",
        "\n",
        "The action ``take()`` takes in an RDD and an integer $n$, and returns the first\n",
        "$n$ items of the RDD.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Input.__  \n",
        "\n",
        "* A RDD $R = [r_i | 0\\leq i \\leq k-1]$.\n",
        "* An integer $n$.\n",
        "\n",
        "__Output.__\n",
        "\n",
        "* The first $n$ items from $R$.\n",
        "\n",
        "\n",
        "ðŸ‘‰ It works by first scanning one partition, and uses the results from that partition to estimate the number of additional partitions needed to satisfy the limit.\n"
      ],
      "metadata": {
        "id": "Z_5m6GZW5_-b"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frtRvo1KXHe-"
      },
      "source": [
        "people_rdd.take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‰ Similar functions are ``takeOrdered()`` (documented [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.takeOrdered.html)) and ``takeSample()`` (documented [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.takeSample.html))."
      ],
      "metadata": {
        "id": "7LWKRYc76Of1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgg64SPLXKsh"
      },
      "source": [
        "## Action ``collect()``\n",
        "\n",
        "The action ``collect()`` takes in a RDD and returns a list containing all the elements of the RDD.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Input.__  \n",
        "\n",
        "* A RDD $R = [r_i | 0\\leq i \\leq k-1]$.\n",
        "\n",
        "__Output.__\n",
        "\n",
        "* A Python list with all the elements of $R$.\n",
        "\n",
        "ðŸ‘‰ All the elements of the RDD are sent to the driver. Depending on the size, this may cause a high network traffic and may result in the overflow of the memory of the server hosting the driver.\n",
        "The use of ``collect()`` is discouraged for large RDDs."
      ],
      "metadata": {
        "id": "firs0FF46pdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "women_rdd.collect()"
      ],
      "metadata": {
        "id": "KP2mbjR6mPNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUBVwUmoqqUZ"
      },
      "source": [
        "## Action ``reduce()``\n",
        "\n",
        "The action ``reduce()`` takes in a RDD $R$ and a function $f$ and returns a value that is obtained by applying function $f$ to all elements of $R$ pairwise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Input.__  \n",
        "\n",
        "* A RDD $R = [r_i | 0\\leq i \\leq k-1]$.\n",
        "* A function $f$ that takes in two arguments.\n",
        "\n",
        "ðŸ‘‰ The two arguments of $f$ __must have the same type__ of the elements of $R$.\n",
        "\n",
        "ðŸ‘‰ Function $f$ must be **commutative** and **associative**.\n",
        "\n",
        "__Output.__ The result is computed in the following way:\n",
        "\n",
        "```\n",
        "result = r_1\n",
        "for i in range(2, k):\n",
        "  result = f(r_i, result)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "uvFqE8gR7bxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following example:\n",
        "\n",
        "* we read the content of file ``./data/moby-dick.txt`` (function ``sc.textFile()``);\n",
        "\n",
        "* we compute the number of characters in each line (function ``map(lambda x: len(x)``);\n",
        "\n",
        "* we sum pairwise the number of characters in each line to obtain the total number of characters in the file (function ``reduce()``).\n",
        "\n",
        "ðŸ‘‰ Note how the transformations are chained together in lines 5-6. This programming style is common in Spark programs.\n"
      ],
      "metadata": {
        "id": "DBMPjdLa7vZO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb2QvVcnqzDz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "54b56248-f4f1-495c-cabc-aa711822dc05"
      },
      "source": [
        "data_file = \"./data/moby-dick.txt\"\n",
        "\n",
        "nb_chars_per_line = sc.textFile(data_file)\\\n",
        "              .map(lambda x: len(x))\n",
        "\n",
        "nb_chars = nb_chars_per_line.reduce(lambda x, y: x + y)\n",
        "print(nb_chars)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c4f32334f8b1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data/moby-dick.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnb_chars_per_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m               \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Action ``saveAsTextFile()``\n",
        "\n",
        "The action ``saveAsTextFile()`` takes in an RDD and a filename and stores the content of the RDD to the file."
      ],
      "metadata": {
        "id": "4gxC2wwHJYqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "women_rdd.saveAsTextFile('./data/women_rdd.txt')"
      ],
      "metadata": {
        "id": "W3Y4dpsQJsc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‰ If you look at your local file system, you'll see that the created file is actually a directory, that contains a file for each partition of the saved RDD.\n",
        "\n",
        "ðŸ‘‰ Similar functions exist to save the file to other formats."
      ],
      "metadata": {
        "id": "KgYPrIbtJ26q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldnZ2q1LurTl"
      },
      "source": [
        "# Pair RDDs\n",
        "\n",
        "In pair RDDs each item is a pair $(k, v)$, where $k$ is referred to as a __key__ and $v$ as a __value__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVp30q2k-8av",
        "scrolled": true
      },
      "source": [
        "pair_rdd = sc.parallelize([(\"cat\", 2), (\"owl\", 3), (\"dog\", 5), (\"cat\", 2), (\"dog\", 1), (\"cow\", 1), (\"cat\", 3), (\"owl\", 4), (\"tiger\", 1)])\n",
        "pair_rdd.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair RDDs accept all the transformations and actions presented above.\n",
        "\n",
        "They also accept transformations and actions that are specifically tailored to handle key-value pairs."
      ],
      "metadata": {
        "id": "fbBPop8kr3i_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hisbHCdDvuFy"
      },
      "source": [
        "## Transformation ``reduceByKey()``\n",
        "\n",
        "The transformation ``reduceByKey()`` takes in a pair RDD $R$ and a commutative and associative reduce function $f$, and returns a new pair RDD with the same keys as $R$; the values associated with each key are merged using $f$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Input.__  \n",
        "\n",
        "* A RDD $R = [(k_i, v_i) | 0\\leq i \\leq n-1, i\\neq j  \\nRightarrow k_i \\neq k_j]$.\n",
        "\n",
        "* A function $f$ that takes two arguments.\n",
        "\n",
        "\n",
        "ðŸ‘‰ The two arguments of $f$ __must have the same type__ of the __values__ (<u>not the keys</u>) in $R$.\n",
        "\n",
        "ðŸ‘‰ $f$ must be **commutative** and **associative**.\n",
        "\n",
        "\n",
        "\n",
        "__Output.__\n",
        "\n",
        "For each key $k$ in $R$, it applies ``reduce()`` to all values associated to $k$.\n",
        "\n",
        "ðŸ‘‰ ``reduce()`` is an action, ``reduceByKey()`` is a transformation.\n",
        "\n"
      ],
      "metadata": {
        "id": "92A8aegpDdt8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey3BL_yvv1lL"
      },
      "source": [
        "print(f\"Content of the input RDD: {pair_rdd.collect()}\")\n",
        "pair_rdd_rbk = pair_rdd.reduceByKey(lambda x, y : x + y)\n",
        "\n",
        "print(f\"Content of the output RDD (after reduceByKey): {pair_rdd_rbk.collect()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMCDh1hXyLi9"
      },
      "source": [
        "## Transformation ``mapValues()``\n",
        "\n",
        "The transformation ``mapValues()`` takes in a pair RDD $R$ and a function $f$ and returns a new pair RDD with the same keys as $R$; the function $f$ is applied to each value.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Input.__  \n",
        "\n",
        "* A RDD $R = [(k_i, v_i) | 0\\leq i \\leq n-1, i\\neq j  \\nRightarrow k_i \\neq k_j]$.\n",
        "\n",
        "* A function $f$.\n",
        "\n",
        "__Output.__\n",
        "\n",
        "* A __new RDD__ $R^\\prime = [(k_i, f(v_i)) | (k_i, v_i) \\in R ]$.\n"
      ],
      "metadata": {
        "id": "cd1UGHlnIQL-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yd21D8ZyZLh"
      },
      "source": [
        "print(f\"Content of the input RDD: {pair_rdd.collect()}\")\n",
        "\n",
        "pair_rdd_mv = pair_rdd.mapValues(lambda x: x+1)\n",
        "\n",
        "print(f\"Content of the output RDD (after mapValues): {pair_rdd_mv.collect()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMMHNRNwxyR0"
      },
      "source": [
        "## Transformation ``groupByKey()``\n",
        "\n",
        "The transformation ``groupByKey()`` takes in a pair RDD $R$ and returns a new pair RDD with the same keys as $R$; the values of each key are grouped  into a single collection.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Input.__  \n",
        "\n",
        "* A RDD $R = [(k_i, v_i) | 0\\leq i \\leq n-1, i\\neq j  \\nRightarrow k_i \\neq k_j]$.\n",
        "\n",
        "__Output.__\n",
        "\n",
        "* A __new RDD__ $R^\\prime = [(k_i, l) | l = [v_j | (k_i, v_j) \\in R ]]$.\n",
        "\n",
        "ðŸ‘‰ ``groupByKey()`` returns an iterable.\n"
      ],
      "metadata": {
        "id": "Atbx5BwmG2BD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua-XfMN9xnhF"
      },
      "source": [
        "print(f\"Content of the input RDD: {pair_rdd.collect()}\")\n",
        "\n",
        "pair_rdd_gbk = pair_rdd.groupByKey()\n",
        "\n",
        "print(f\"Content of the output RDD (after groupByKey): {pair_rdd_gbk.collect()}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can still convert each iterable to a list by using the transformation ``mapValues()``."
      ],
      "metadata": {
        "id": "rba3Ll1nHV_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pair_rdd_gbk_list = pair_rdd_gbk.mapValues(lambda it: list(it))\n",
        "\n",
        "print(f\"Content of the output RDD (after mapValues): {pair_rdd_gbk_list.collect()}\")"
      ],
      "metadata": {
        "id": "Xv-M7Q2WHP7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformation ``join()``\n",
        "\n",
        "The transformation ``join()`` takes in two pair RDDs and creates a new RDD where the values associated with the same key are joined together.\n",
        "\n"
      ],
      "metadata": {
        "id": "5Hv5rJzyK7Gu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Input.__  \n",
        "\n",
        "* A RDD $R_1 = [(k^1_i, v^1_i) | 0\\leq i \\leq n-1, i\\neq j  \\nRightarrow k^1_i \\neq k^1_j]$.\n",
        "\n",
        "* A RDD $R_2 = [(k^2_i, v^2_i) | 0\\leq i \\leq m-1, i\\neq j  \\nRightarrow k^2_i \\neq k^2_j]$.\n",
        "\n",
        "__Output.__\n",
        "\n",
        "* A __new RDD__ $R^\\prime = [(j, (v^1, v^2)) | \\forall\\ (j, v^1)\\in R_1 \\ \\forall\\ (j, v^2) \\in R_2]]$.\n",
        "\n",
        "ðŸ‘‰ If a key occurs in $R_1$ but does not occur in $R_2$ (and the other way round), that key is not represented in the output of ``join()``.\n"
      ],
      "metadata": {
        "id": "TmFx1BHmLYeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pair_rdd_text = sc.parallelize([(\"cat\", \"meow\"), (\"dog\", \"woof\"), (\"dog\", \"arf\"), (\"cow\", \"moo\")])\n",
        "\n",
        "print(f\"Content of the first RDD: {pair_rdd.collect()}\")\n",
        "print(f\"Content of the second RDD: {pair_rdd_text.collect()}\")\n",
        "\n",
        "pair_rdd_join = pair_rdd.join(pair_rdd_text)\n",
        "\n",
        "print(f\"Content of the output RDD (after join): {pair_rdd_join.collect()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jy1_crTLMouu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Narrow and wide transformations\n",
        "\n",
        "Some transformations apply a computation on an RDD without the need of moving data from one server to another server.\n",
        "For example, the transformation ``map()`` applies a function $f$ to all items of an input RDD and does not need to move the items from one partition to another partition in doing so.\n",
        "In other words, the contents of partition $i$ of the output RDD is determined by applying function $f$ on all items of partition $i$ of the input RDD. The operation is local to each single partition.\n",
        "\n",
        "In general, a **narrow transformation** is one where each partition of the output RDD is computed from one single partition of the input RDD.\n",
        "Examples of narrow transformations are: ``map``, ``flatMap``, ``filter`` and ``union``.\n",
        "\n",
        "Other transformations need to move items from one partition to another partition (hence, from one server to another server).\n",
        "\n",
        "ðŸ‘‰ The action of moving data from one partition to another partition is called **shuffle**.\n",
        "\n",
        "For example, ``reduceByKey()`` needs to move to the same partition the pairs sharing the same key.\n",
        "\n",
        "In general, a **wide transformation** in one that triggers a data shuffle.\n",
        "Examples of wide transformations are: ``distinct()``, ``intersection()``, ``join()``, ``reduceByKey()`` and ``groupByKey()``.\n"
      ],
      "metadata": {
        "id": "i1tPO8VahwD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example: word count\n",
        "\n",
        "The following code defines a function ``wordcount()`` that counts the number of occurrences of each word in a given file.\n",
        "The comments should help you understand the code.\n",
        "\n",
        "ðŸ‘‰ Note how the transformations are chained. This is a common coding style in Spark programs."
      ],
      "metadata": {
        "id": "6ab8uJAwSQsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wordcount(content_file, stopwords_file):\n",
        "\n",
        "  '''\n",
        "  Counts the number of occurrences of the words in a given file (first argument).\n",
        "  The function ignores stop words (insignificant words, such as articles, prepositions)\n",
        "  that are specified in a file (second argument).\n",
        "  The returned words are sorted by decreasing frequency (frequent words come first).\n",
        "\n",
        "  Parameters\n",
        "  -----------\n",
        "    content_file (str): path to the file with the text to analyze.\n",
        "    stopwords_file (str): path to the file that contains the stopwords.\n",
        "\n",
        "\n",
        "  Returns\n",
        "  --------\n",
        "    rdd.\n",
        "      An RDD of pairs (w, ow), where w is a word and ow is the number of occurrences\n",
        "      of the word in content_file.\n",
        "  '''\n",
        "\n",
        "  # Read the stopwords from the given file into a list.\n",
        "  stopwords = []\n",
        "  with open(stopwords_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        stopwords.append(line.lower().strip())\n",
        "  '''\n",
        "  Here is the meaning of each transformation:\n",
        "\n",
        "  textFile: Read the content of the file into a RDD.\n",
        "  flatMap: Split each line into its constituent words.\n",
        "  filter: Remove stop words.\n",
        "  map: Associate the value 1 to each word\n",
        "  reduceByKey: For each word, sum up all the 1s associated to that word.\n",
        "  sortBy: Sort by decreasing frenquency and return the result.\n",
        "  '''\n",
        "  return sc.textFile(content_file)\\\n",
        "            .flatMap(lambda x: x.split())\\\n",
        "            .map(lambda x: x.lower())\\\n",
        "            .filter(lambda x: x not in stopwords)\\\n",
        "            .map(lambda x: (x, 1))\\\n",
        "            .reduceByKey(lambda x, y: x+y)\\\n",
        "            .sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "# Call the function. The function returns an RDD.\n",
        "word_counts = wordcount('./data/moby-dick.txt', './data/stopwords.txt')\n",
        "\n",
        "# Show the first ten items in the RDD returned by the function.\n",
        "word_counts.take(10)\n"
      ],
      "metadata": {
        "id": "6FpI1S0SScPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Good to know.** In the previous program, calling the function ``wordcount()`` does not trigger the execution of the transformations. The transformations are only called when we call the first action on the RDD returned by ``wordcount()``. This execution mode is referred to as **lazy evaluation**.\n",
        "\n",
        "ðŸ‘‰ Always remember to call an action on an RDD is you want the transformations on that RDD to be executed.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jvAhWdenfm6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "* Spark is a distributed computing framework used to develop applications that can execute **parallel computations** on data distributed across servers in a **cluster**.\n",
        "\n",
        "* Parallel computations on distributed data are implemented as operations on data structures called **Resilient Distributed Datasets (RDDs)**.\n",
        "\n",
        "* RDDs (Resilient Distributed Datasets) are **immutable**, **distributed** collections of items (e.g., integer, strings, objects).\n",
        "\n",
        "* Items in an RDD are divided into **disjoint partitions**. A partition resides on a single server; two distinct partitions can reside on the same server.\n",
        "\n",
        "* A Spark program creates a new **RDD**, either from an in-memory collection (for debugging or prototyping purposes) or from external sources (production environments).\n",
        "\n",
        "* A Spark program manipulates RDDs through functions that are called **transformations**. A transformation takes in one or several RDDs and returns a new RDD. A tranformation never modifies the contents of the input RDDs. A transformation applies a computation on an input RDD through parallel computational tasks that operate on each partition of the RDD.\n",
        "\n",
        "* **Wide** are costlier than **narrow** transformations, as they always trigger a data shuffle (move data from one partition to another).\n",
        "\n",
        "* The result of a Spark program is obtained by calling an **action**, that is a function that takes in one RDD and returns a value.\n",
        "Only when an action on a RDD is invoke are the transformations on that RDD actually executed (**lazy evaluation**).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7fO3WnS0dFDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced considerations\n",
        "\n",
        "The way items are partitioned in a RDD may impact the performances of transformations and actions.\n",
        "\n",
        "Let's illustrate this point with an example.\n",
        "\n"
      ],
      "metadata": {
        "id": "dDMnnuTWN_na"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell, we force Spark to create four partitions of a new RDD by adding a second argument to the function ``parallelize()``."
      ],
      "metadata": {
        "id": "yxMXl52bSuA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = [(\"cat\", 2), (\"owl\", 3), (\"dog\", 5), (\"cat\", 2), (\"dog\", 1), (\"cow\", 1), (\"cat\", 3), (\"owl\", 4), (\"tiger\", 1)]\n",
        "\n",
        "rdd_l1 = sc.parallelize(l1, 4)\n",
        "\n",
        "print(f\"Number of partitions of rdd_l: {rdd_l1.getNumPartitions()}\")\n",
        "\n",
        "for i, partition in enumerate(rdd_l1.glom().collect()):\n",
        "  print(f\"Content of partition {i}: {partition}\")\n"
      ],
      "metadata": {
        "id": "lQpfMOKbR5B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‰ Two items with the same key are not necessarily in the same partition."
      ],
      "metadata": {
        "id": "zifO2nxsV6e0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create another RDD, where items share some of the keys with the RDD created above."
      ],
      "metadata": {
        "id": "vOf6hP1vWT6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_l2 = sc.parallelize([(\"cat\", \"meow\"), (\"dog\", \"woof\"), (\"dog\", \"arf\"), (\"cow\", \"moo\")])\n",
        "\n",
        "print(f\"Number of partitions of rdd_l: {rdd_l2.getNumPartitions()}\")\n",
        "\n",
        "for i, partition in enumerate(rdd_l2.glom().collect()):\n",
        "  print(f\"Content of partition {i}: {partition}\")\n"
      ],
      "metadata": {
        "id": "8H9ecLZ6Ttfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code joins the two RDDs."
      ],
      "metadata": {
        "id": "_UuyPilrWrR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_join = rdd_l1.join(rdd_l2)\n",
        "\n",
        "print(f\"Number of partitions of rdd_join: {rdd_join.getNumPartitions()}\")\n",
        "\n",
        "for i, partition in enumerate(rdd_join.glom().collect()):\n",
        "  print(f\"Content of partition {i}: {partition}\")\n"
      ],
      "metadata": {
        "id": "QEJ5NjfmUeyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following figure shows what happens when we join the two RDDs.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=11H7wYqMf_SauPHk4-qoFmoPF8kmm7Xnq\" width=\"800\"  /></center>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Data of <u>both RDD</u> need to be shuffled. More precisely:\n",
        "\n",
        "1. The pairs sharing the same key in the first RDD are moved to the same partition.\n",
        "\n",
        "2. The pairs sharing the same key in the second RDD are moved to the same partition.\n",
        "\n",
        "ðŸ‘‰ The destination partition of a key-value pair is computed by applying a hash function on the key; therefore, if two pairs have the same key, they will land on the same output partition, regardless of whether they belong to the same input RDD or not.\n",
        "\n",
        "3. Now that all pairs from both RDDs sharing the same key  are in the same partition, they can be joined.\n",
        "\n",
        "ðŸ‘‰ Two different partitions may reside on two different servers. Therefore, copying data from one partition to another leads to data being shuffled on the network. **This slows down the computation**.\n",
        "\n"
      ],
      "metadata": {
        "id": "IvJoWhraXFMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we know that the RDD ``rdd_l1`` will be joined many times with other RDDs, it is worth taking the time to **hash-partition** ``rdd_l1`` before calling the join transformations.\n",
        "\n",
        "ðŸ‘‰ A RDD is **hash-partitioned** when any two pairs sharing the same key are in the same partition (hence, they are stored on the same server).\n",
        "\n",
        "The following code shows how to hash partition an RDD."
      ],
      "metadata": {
        "id": "OigpO21hkdA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new HashPartitioner\n",
        "class HashPartitioner:\n",
        "    def __init__(self, num_partitions):\n",
        "        self.num_partitions = num_partitions\n",
        "\n",
        "    def __call__(self, key):\n",
        "        return hash(key) % self.num_partitions\n",
        "\n",
        "rdd_l1_hash = rdd_l1.partitionBy(4, HashPartitioner(4))\n",
        "\n",
        "\n",
        "print(f\"Number of partitions of rdd_l1_hash: {rdd_l1_hash.getNumPartitions()}\")\n",
        "\n",
        "for i, partition in enumerate(rdd_l1_hash.glom().collect()):\n",
        "  print(f\"Content of partition {i}: {partition}\")\n"
      ],
      "metadata": {
        "id": "lkDHxUBgWxuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, if we try to join ``rdd_l1`` with ``rdd_l2``, only the pairs of ``rdd_l2`` will be shuffled, which results is a lower execution time.\n",
        "\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1a9emjO6h9Ld8TtgvDCY9ahKnwFpRKBi4\" width=\"800\"  /></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "pSX9eATylyqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About hash partitioning\n",
        "\n",
        "It is not necessary to explicitly hash-partition an RDD if we want to join this RDD to another one only once.\n",
        "In general, we should hash-partition an RDD when:\n",
        "\n",
        "* the RDD is large ;\n",
        "\n",
        "* we intend to join the RDD with other RDDs many times.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "quFvUWQ-psOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that any RDD obtained from joining two RDDs is **hash-partitioned**.\n",
        "\n",
        "The trasformations that hash-partition their output RDDs are: ``cogroup()``, ``groupWith()``, ``join()``, ``leftOuterJoin()``, ``rightOuterJoin()``, ``groupByKey()``, ``reduceByKey()``, ``combineByKey()``, ``partitionBy()``, ``sort()``.\n",
        "\n",
        "The following transformations: ``mapValues()``, ``flatMapValues()`` and ``filter()`` guarantee that the output is hash-partitioned if their input is.\n"
      ],
      "metadata": {
        "id": "ZWP_HugsqpUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The case of ``map`` and ``mapValues``\n",
        "\n",
        "Consider again the RDD ``rdd_l1``.\n",
        "\n"
      ],
      "metadata": {
        "id": "LiB6h8sxZp31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_l1.collect()"
      ],
      "metadata": {
        "id": "njezpu45r4Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This RDD is not hash-partitioned. We can also confirm it by checking that the partitioner associated with the RDD is ``None``."
      ],
      "metadata": {
        "id": "9CNyt7GDY4ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Partitioner associated with the rdd_l1: {rdd_l1.partitioner}\")"
      ],
      "metadata": {
        "id": "jGnlj_sIP6gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's apply a ``reduceByKey()`` transformation.\n",
        "\n",
        "The RDD returned by the transformation ``reduceByKey()`` has a partitioner associated."
      ],
      "metadata": {
        "id": "rUsq-T9pQEQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_l1_rbk = rdd_l1.reduceByKey(lambda x, y: x+y)\n",
        "\n",
        "print(f\"Partitioner associated with the rdd_l1_rbk: {rdd_l1_rbk.partitioner}\")\n",
        "print(f\"Content of the rdd_l1_rbk: {rdd_l1_rbk.collect()}\")"
      ],
      "metadata": {
        "id": "S1xQivijQKlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell, we apply to ``rdd_l1_rbk`` (the result of ``reduceByKey()``) a transformation ``map()`` that does not change the keys of the RDD.\n",
        "\n",
        "Which partitioner is associated to the result of ``map()``?"
      ],
      "metadata": {
        "id": "m7Fp5isIQlX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_l1_rbk_map = rdd_l1_rbk.map(lambda x: (x[0], x[1] + 1))\n",
        "\n",
        "print(f\"Partitioner associated with the rdd_l1_rbk_map: {rdd_l1_rbk_map.partitioner}\")\n",
        "print(f\"Content of the rdd_l1_rbk_map: {rdd_l1_rbk_map.collect()}\")"
      ],
      "metadata": {
        "id": "cA78WCdzQwlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, look at what happens when we use the transformation ``mapValues()`` instead of the transformation ``map()``."
      ],
      "metadata": {
        "id": "Cng3mUvnRRd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_l1_rbk_mapvalues = rdd_l1_rbk.mapValues(lambda x: x+1)\n",
        "\n",
        "print(f\"Partitioner associated with the rdd_l1_rbk_mapvalues: {rdd_l1_rbk_mapvalues.partitioner}\")\n",
        "print(f\"Content of the rdd_l1_rbk_mapvalues: {rdd_l1_rbk_mapvalues.collect()}\")"
      ],
      "metadata": {
        "id": "yKFy-jpKRYze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Good to know.** The ``map()`` transformation does not preserve the partitioner associated to the input RDD. In fact, we pass ``map()`` a function for which Spark has no way to know whether it modifies the keys or not.\n",
        "As opposed to that, ``mapValues()`` is a transformation that never touches the keys of the input RDD, therefore, Spark can confidently preserve the partitioner associated with the input RDD.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Dxpu0HL-Rh1T"
      }
    }
  ]
}